"""Code review and evaluation agent."""

import json
from typing import Dict, Any, List, Optional
from pathlib import Path

from .base_agent import BaseAgent
from models import Task, TaskType, TaskStatus, MessageType
from config import Settings
from tools import FileTools, CodeTools, TestTools


class ReviewerAgent(BaseAgent):
    """
    ä»£ç è¯„ä¼°æ™ºèƒ½ä½“
    
    è´Ÿè´£ï¼š
    - ä»£ç è´¨é‡å®¡æŸ¥
    - åŠŸèƒ½æµ‹è¯•
    - å®‰å…¨æ£€æŸ¥
    - æ€§èƒ½è¯„ä¼°
    - åé¦ˆä¸å»ºè®®
    """
    
    def __init__(self, settings: Settings, file_tools: FileTools):
        super().__init__(
            agent_id="reviewer",
            name="ä»£ç è¯„ä¼°æ™ºèƒ½ä½“",
            settings=settings
        )
        self.file_tools = file_tools
        self.code_tools = CodeTools()
        self.test_tools = TestTools()
        self.project_path: Optional[Path] = None
    
    def set_project_path(self, path: Path):
        """è®¾ç½®é¡¹ç›®è·¯å¾„"""
        self.project_path = path
    
    def get_system_prompt(self) -> str:
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥ä¸“å®¶å’Œè´¨é‡ä¿è¯å·¥ç¨‹å¸ˆã€‚ä½ çš„èŒè´£æ˜¯ï¼š

1. å®¡æŸ¥ä»£ç è´¨é‡å’Œæœ€ä½³å®è·µ
2. æ£€æµ‹æ½œåœ¨çš„ bug å’Œå®‰å…¨æ¼æ´
3. è¯„ä¼°ä»£ç çš„å¯ç»´æŠ¤æ€§å’Œå¯è¯»æ€§
4. éªŒè¯åŠŸèƒ½æ˜¯å¦ç¬¦åˆéœ€æ±‚
5. æä¾›æ”¹è¿›å»ºè®®

å®¡æŸ¥ç»´åº¦ï¼š
- ä»£ç é£æ ¼ï¼šæ˜¯å¦éµå¾ªè¯­è¨€è§„èŒƒ
- åŠŸèƒ½æ­£ç¡®æ€§ï¼šæ˜¯å¦å®ç°äº†é¢„æœŸåŠŸèƒ½
- å®‰å…¨æ€§ï¼šæ˜¯å¦æœ‰å®‰å…¨éšæ‚£
- æ€§èƒ½ï¼šæ˜¯å¦æœ‰æ€§èƒ½é—®é¢˜
- å¯ç»´æŠ¤æ€§ï¼šä»£ç æ˜¯å¦æ˜“äºç†è§£å’Œä¿®æ”¹

è¾“å‡ºæ ¼å¼ï¼š
è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºå®¡æŸ¥ç»“æœï¼ŒåŒ…å«ï¼š
- score: æ€»ä½“è¯„åˆ† (1-10)
- issues: é—®é¢˜åˆ—è¡¨
- suggestions: æ”¹è¿›å»ºè®®
- summary: æ€»ç»“"""
    
    async def execute_task(self, task: Task) -> Dict[str, Any]:
        """æ‰§è¡Œå®¡æŸ¥ä»»åŠ¡"""
        self.start_task(task)
        
        try:
            if task.type == TaskType.REVIEW:
                result = await self._review_code(task)
            elif task.type == TaskType.TESTING:
                result = await self._run_tests(task)
            else:
                result = await self._generic_review(task)
            
            self.complete_task(result)
            return result
            
        except Exception as e:
            self.fail_task(str(e))
            return {"error": str(e)}
    
    async def _review_code(self, task: Task) -> Dict[str, Any]:
        """å®¡æŸ¥ä»£ç """
        review_type = task.input_data.get("review_type", "full")
        project_spec = task.input_data.get("project_spec", {})
        
        if not self.project_path:
            return {"error": "é¡¹ç›®è·¯å¾„æœªè®¾ç½®"}
        
        # è·å–æ‰€æœ‰æ–‡ä»¶
        files = self.file_tools.list_files("", self.project_path, "*")
        
        # è¿‡æ»¤ä»£ç æ–‡ä»¶
        code_files = [f for f in files if self._is_code_file(f)]
        
        # å®¡æŸ¥æ¯ä¸ªæ–‡ä»¶
        file_reviews = []
        total_score = 0
        all_issues = []
        all_suggestions = []
        
        for file_path in code_files:
            content = self.file_tools.read_file(file_path, self.project_path)
            if content:
                review = await self._review_single_file(file_path, content, project_spec)
                file_reviews.append(review)
                total_score += review.get("score", 5)
                all_issues.extend(review.get("issues", []))
                all_suggestions.extend(review.get("suggestions", []))
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_score = total_score / len(file_reviews) if file_reviews else 0
        
        # è¿è¡Œè¯­æ³•æ£€æŸ¥
        syntax_issues = self._check_syntax_all(code_files)
        all_issues.extend(syntax_issues)
        
        return {
            "overall_score": round(avg_score, 1),
            "files_reviewed": len(file_reviews),
            "file_reviews": file_reviews,
            "total_issues": len(all_issues),
            "issues": all_issues[:20],  # é™åˆ¶æ•°é‡
            "suggestions": list(set(all_suggestions))[:10],
            "passed": avg_score >= 6 and len([i for i in all_issues if i.get("severity") == "critical"]) == 0
        }
    
    async def _review_single_file(self, file_path: str, content: str, 
                                   project_spec: Dict) -> Dict[str, Any]:
        """å®¡æŸ¥å•ä¸ªæ–‡ä»¶"""
        
        prompt = f"""è¯·å®¡æŸ¥ä»¥ä¸‹ä»£ç æ–‡ä»¶ï¼š

## æ–‡ä»¶è·¯å¾„
{file_path}

## é¡¹ç›®éœ€æ±‚
{json.dumps(project_spec.get("features", []), ensure_ascii=False)}

## ä»£ç å†…å®¹
```
{content[:3000]}  
```
{"(ä»£ç å·²æˆªæ–­)" if len(content) > 3000 else ""}

## è¯·ä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œå®¡æŸ¥å¹¶ä»¥ JSON æ ¼å¼è¿”å›ï¼š

```json
{{
    "score": è¯„åˆ†(1-10),
    "issues": [
        {{
            "type": "bug/style/security/performance",
            "severity": "critical/high/medium/low",
            "line": è¡Œå·æˆ–null,
            "message": "é—®é¢˜æè¿°"
        }}
    ],
    "suggestions": ["æ”¹è¿›å»ºè®®1", "æ”¹è¿›å»ºè®®2"],
    "summary": "æ€»ä½“è¯„ä»·"
}}
```"""

        messages = [
            {"role": "system", "content": self.get_system_prompt()},
            {"role": "user", "content": prompt}
        ]
        
        response = await self.call_llm(messages, temperature=self.settings.reviewer_temperature)
        
        # è§£æå“åº”
        review = self._parse_review_response(response)
        review["file"] = file_path
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤è¯„å®¡
        if review.get("score", 0) == 0:
            review = self._get_default_review(file_path, content)
        
        return review
    
    def _parse_review_response(self, response: str) -> Dict[str, Any]:
        """è§£æå®¡æŸ¥å“åº”"""
        import re
        
        try:
            return json.loads(response)
        except:
            pass
        
        # å°è¯•æå– JSON
        json_match = re.search(r'\{[\s\S]*\}', response)
        if json_match:
            try:
                return json.loads(json_match.group())
            except:
                pass
        
        return {
            "score": 0,
            "issues": [],
            "suggestions": [],
            "summary": "æ— æ³•è§£æå®¡æŸ¥ç»“æœ"
        }
    
    def _get_default_review(self, file_path: str, content: str) -> Dict[str, Any]:
        """è·å–é»˜è®¤å®¡æŸ¥ç»“æœ"""
        issues = []
        suggestions = []
        score = 7
        
        # åŸºæœ¬æ£€æŸ¥
        lines = content.split('\n')
        
        # æ£€æŸ¥æ–‡ä»¶é•¿åº¦
        if len(lines) > 500:
            issues.append({
                "type": "style",
                "severity": "medium",
                "line": None,
                "message": f"æ–‡ä»¶è¿‡é•¿ ({len(lines)} è¡Œ)ï¼Œå»ºè®®æ‹†åˆ†"
            })
            score -= 0.5
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ docstring/æ³¨é‡Š
        if file_path.endswith('.py'):
            if '"""' not in content and "'''" not in content:
                issues.append({
                    "type": "style",
                    "severity": "low",
                    "line": None,
                    "message": "ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²"
                })
                suggestions.append("æ·»åŠ æ¨¡å—å’Œå‡½æ•°çš„æ–‡æ¡£å­—ç¬¦ä¸²")
                score -= 0.5
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯
        sensitive_patterns = ['password', 'secret', 'api_key', 'token']
        for pattern in sensitive_patterns:
            if pattern in content.lower() and '=' in content:
                issues.append({
                    "type": "security",
                    "severity": "high",
                    "line": None,
                    "message": f"å¯èƒ½å­˜åœ¨ç¡¬ç¼–ç çš„æ•æ„Ÿä¿¡æ¯: {pattern}"
                })
                score -= 1
        
        # æ£€æŸ¥ TODO/FIXME
        todo_count = content.lower().count('todo') + content.lower().count('fixme')
        if todo_count > 0:
            issues.append({
                "type": "style",
                "severity": "low",
                "line": None,
                "message": f"å­˜åœ¨ {todo_count} ä¸ª TODO/FIXME æ³¨é‡Š"
            })
        
        return {
            "file": file_path,
            "score": max(1, min(10, score)),
            "issues": issues,
            "suggestions": suggestions,
            "summary": f"ä»£ç åŸºæœ¬ç¬¦åˆè§„èŒƒï¼Œè¯„åˆ† {score}/10"
        }
    
    def _check_syntax_all(self, files: List[str]) -> List[Dict[str, Any]]:
        """æ£€æŸ¥æ‰€æœ‰æ–‡ä»¶çš„è¯­æ³•"""
        issues = []
        
        for file_path in files:
            content = self.file_tools.read_file(file_path, self.project_path)
            if not content:
                continue
            
            result = self.test_tools.check_file_syntax(file_path, content)
            
            if not result["valid"]:
                for error in result["errors"]:
                    issues.append({
                        "type": "bug",
                        "severity": "critical",
                        "file": file_path,
                        "message": f"è¯­æ³•é”™è¯¯: {error}"
                    })
            
            for warning in result.get("warnings", []):
                issues.append({
                    "type": "style",
                    "severity": "low",
                    "file": file_path,
                    "message": f"è­¦å‘Š: {warning}"
                })
        
        return issues
    
    def _is_code_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯ä»£ç æ–‡ä»¶"""
        code_extensions = {
            '.py', '.js', '.jsx', '.ts', '.tsx', 
            '.html', '.htm', '.css', '.scss',
            '.json', '.yaml', '.yml'
        }
        return Path(file_path).suffix.lower() in code_extensions
    
    async def _run_tests(self, task: Task) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•"""
        test_type = task.input_data.get("test_type", "functional")
        features = task.input_data.get("features", [])
        
        if not self.project_path:
            return {"error": "é¡¹ç›®è·¯å¾„æœªè®¾ç½®"}
        
        results = {
            "test_type": test_type,
            "passed": True,
            "tests_run": 0,
            "tests_passed": 0,
            "tests_failed": 0,
            "details": []
        }
        
        # åŠŸèƒ½æµ‹è¯•
        if test_type == "functional":
            # æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = self._get_required_files(features)
            for file_path in required_files:
                exists = self.file_tools.file_exists(file_path, self.project_path)
                results["tests_run"] += 1
                
                if exists:
                    results["tests_passed"] += 1
                    results["details"].append({
                        "test": f"æ–‡ä»¶å­˜åœ¨: {file_path}",
                        "passed": True
                    })
                else:
                    results["tests_failed"] += 1
                    results["passed"] = False
                    results["details"].append({
                        "test": f"æ–‡ä»¶å­˜åœ¨: {file_path}",
                        "passed": False,
                        "error": "æ–‡ä»¶ä¸å­˜åœ¨"
                    })
            
            # æ£€æŸ¥å…³é”®åŠŸèƒ½
            func_results = await self._check_features(features)
            results["tests_run"] += func_results["total"]
            results["tests_passed"] += func_results["passed"]
            results["tests_failed"] += func_results["failed"]
            results["details"].extend(func_results["details"])
            
            if func_results["failed"] > 0:
                results["passed"] = False
        
        # è¿è¡Œ pytestï¼ˆå¦‚æœå­˜åœ¨æµ‹è¯•æ–‡ä»¶ï¼‰
        test_files = self.file_tools.list_files("", self.project_path, "test_*.py")
        if test_files:
            pytest_results = self.test_tools.run_python_tests(
                ".",
                self.project_path
            )
            results["pytest"] = pytest_results
        
        return results
    
    def _get_required_files(self, features: List[str]) -> List[str]:
        """æ ¹æ®åŠŸèƒ½ç¡®å®šå¿…éœ€çš„æ–‡ä»¶"""
        required = ["main.py"]
        
        for feature in features:
            feature_lower = feature.lower()
            
            if "åˆ†ç±»" in feature or "å¯¼èˆª" in feature:
                required.extend([
                    "templates/index.html",
                    "templates/category.html"
                ])
            
            if "è®ºæ–‡" in feature or "è¯¦æƒ…" in feature:
                required.extend([
                    "templates/paper.html",
                    "arxiv_client.py"
                ])
            
            if "æ ·å¼" in feature or "ç•Œé¢" in feature:
                required.append("static/css/style.css")
        
        return list(set(required))
    
    async def _check_features(self, features: List[str]) -> Dict[str, Any]:
        """æ£€æŸ¥åŠŸèƒ½å®ç°"""
        results = {
            "total": 0,
            "passed": 0,
            "failed": 0,
            "details": []
        }
        
        # è¯»å–ä¸»æ–‡ä»¶æ£€æŸ¥è·¯ç”±
        main_content = self.file_tools.read_file("main.py", self.project_path)
        
        if not main_content:
            results["total"] = 1
            results["failed"] = 1
            results["details"].append({
                "test": "main.py å­˜åœ¨ä¸”å¯è¯»",
                "passed": False,
                "error": "æ— æ³•è¯»å– main.py"
            })
            return results
        
        # æ£€æŸ¥è·¯ç”±å®šä¹‰
        route_checks = [
            ('/', 'é¦–é¡µè·¯ç”±'),
            ('/category/', 'åˆ†ç±»è·¯ç”±'),
            ('/paper/', 'è®ºæ–‡è¯¦æƒ…è·¯ç”±'),
        ]
        
        for route, name in route_checks:
            results["total"] += 1
            if route in main_content or f'"{route}' in main_content:
                results["passed"] += 1
                results["details"].append({
                    "test": f"{name} ({route})",
                    "passed": True
                })
            else:
                results["failed"] += 1
                results["details"].append({
                    "test": f"{name} ({route})",
                    "passed": False,
                    "error": f"æœªæ‰¾åˆ°è·¯ç”± {route}"
                })
        
        # æ£€æŸ¥å…³é”®åŠŸèƒ½
        feature_checks = [
            ('bibtex', 'BibTeX åŠŸèƒ½'),
            ('pdf', 'PDF é“¾æ¥åŠŸèƒ½'),
            ('category', 'åˆ†ç±»åŠŸèƒ½'),
        ]
        
        for keyword, name in feature_checks:
            results["total"] += 1
            if keyword.lower() in main_content.lower():
                results["passed"] += 1
                results["details"].append({
                    "test": name,
                    "passed": True
                })
            else:
                results["failed"] += 1
                results["details"].append({
                    "test": name,
                    "passed": False,
                    "error": f"æœªæ‰¾åˆ° {keyword} ç›¸å…³å®ç°"
                })
        
        return results
    
    async def _generic_review(self, task: Task) -> Dict[str, Any]:
        """é€šç”¨å®¡æŸ¥ä»»åŠ¡"""
        return {
            "status": "completed",
            "task_name": task.name,
            "review_type": "generic"
        }
    
    def generate_review_report(self, review_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆå®¡æŸ¥æŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("ä»£ç å®¡æŸ¥æŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")
        
        # æ€»ä½“è¯„åˆ†
        score = review_results.get("overall_score", 0)
        report.append(f"ğŸ“Š æ€»ä½“è¯„åˆ†: {score}/10")
        report.append(f"ğŸ“ å®¡æŸ¥æ–‡ä»¶æ•°: {review_results.get('files_reviewed', 0)}")
        report.append(f"âš ï¸  å‘ç°é—®é¢˜æ•°: {review_results.get('total_issues', 0)}")
        report.append(f"âœ… å®¡æŸ¥ç»“æœ: {'é€šè¿‡' if review_results.get('passed') else 'æœªé€šè¿‡'}")
        report.append("")
        
        # é—®é¢˜åˆ—è¡¨
        issues = review_results.get("issues", [])
        if issues:
            report.append("-" * 40)
            report.append("é—®é¢˜åˆ—è¡¨:")
            report.append("-" * 40)
            for i, issue in enumerate(issues, 1):
                severity_icon = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(
                    issue.get("severity", "low"), "âšª"
                )
                report.append(f"{i}. {severity_icon} [{issue.get('type', 'unknown')}] {issue.get('message', '')}")
                if issue.get("file"):
                    report.append(f"   æ–‡ä»¶: {issue['file']}")
            report.append("")
        
        # æ”¹è¿›å»ºè®®
        suggestions = review_results.get("suggestions", [])
        if suggestions:
            report.append("-" * 40)
            report.append("æ”¹è¿›å»ºè®®:")
            report.append("-" * 40)
            for i, suggestion in enumerate(suggestions, 1):
                report.append(f"{i}. ğŸ’¡ {suggestion}")
            report.append("")
        
        report.append("=" * 60)
        
        return "\n".join(report)

